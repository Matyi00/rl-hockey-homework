{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dreamer as dm\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from importlib import reload\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath('../hockey_env/hockey'))\n",
    "import hockey_env as h_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(-0.2993589396934295)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#env = h_env.HockeyEnv()\n",
    "env = h_env.HockeyEnv(mode=h_env.HockeyEnv.TRAIN_SHOOTING)\n",
    "_ = env.render()\n",
    "\n",
    "dreamer = dm.DreamerV3(\n",
    "\tenv=env,\n",
    "    render=True,\n",
    "    obs_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.shape[0] // 2,\n",
    "    latent_dim=32,\n",
    "    latent_categories_size=32,\n",
    "    model_dim=256,\n",
    "    num_blocks=8,\n",
    "    imagination_horizon=15,\n",
    "    capacity=10000,\n",
    "    replay_ratio=32,\n",
    "    imag_horizon=15,\n",
    "    bins=15,\n",
    "    min_reward=-5,\n",
    "    max_reward=5,\n",
    "    device=device)  \n",
    "\n",
    "replay_ratio = 32\n",
    "\n",
    "number_of_training_steps = 20000\n",
    "batch_size = 16\n",
    "seq_len = 64\n",
    "number_of_trajectories = 25\n",
    "max_steps = 100\n",
    "\n",
    "env_step_per_training_step = batch_size * seq_len // replay_ratio \n",
    "start_samples = 1200\n",
    "\n",
    "world_losses = []\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "rewards = []\n",
    "\n",
    "dreamer.generate_samples(start_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Reward:-0.2679 Losses: 6.89 0.00 0.00    2.78   -0.47\n",
      "Step 1 Reward:-0.2543 Losses: 7.18 0.00 0.00    2.61   0.23\n",
      "Step 2 Reward:-0.3237 Losses: 6.78 0.00 0.00    2.41   0.95\n",
      "Step 3 Reward:-0.3500 Losses: 5.70 0.00 0.00    2.16   1.53\n",
      "Step 4 Reward:-0.2845 Losses: 5.68 0.01 0.00    1.85   1.95\n",
      "Step 5 Reward:-0.4127 Losses: 5.19 0.01 0.00    1.45   1.43\n",
      "Step 6 Reward:-0.2285 Losses: 3.32 0.02 0.00    0.98   0.60\n",
      "Step 7 Reward:-0.0601 Losses: 3.71 0.05 0.00    0.61   0.04\n",
      "Step 8 Reward:-0.2128 Losses: 5.28 0.07 0.01    0.42   -0.19\n",
      "Step 9 Reward:-0.3266 Losses: 4.23 0.08 0.01    0.34   -0.29\n",
      "Step 10 Reward:-0.2305 Losses: 3.21 0.07 0.01    0.32   -0.42\n",
      "Step 11 Reward:-0.1065 Losses: 4.14 0.05 0.00    0.32   -0.63\n",
      "Step 12 Reward:-0.3456 Losses: 5.53 0.03 0.00    0.36   -0.80\n",
      "Step 13 Reward:-0.4462 Losses: 4.19 0.02 0.00    0.42   -1.23\n",
      "Step 14 Reward:-0.3634 Losses: 4.10 0.02 0.00    0.52   -1.78\n",
      "Step 15 Reward:-0.5411 Losses: 5.75 0.01 0.00    0.68   -2.58\n",
      "Step 16 Reward:-0.5358 Losses: 5.45 0.01 0.00    0.85   -3.51\n",
      "Step 17 Reward:-0.2450 Losses: 4.22 0.01 0.00    1.04   -4.59\n",
      "Step 18 Reward:-0.2315 Losses: 3.83 0.01 0.00    1.17   -5.43\n",
      "Step 19 Reward:-0.2171 Losses: 4.64 0.01 0.00    1.27   -6.17\n",
      "Step 20 Reward: 0.2127 Losses: 7.57 0.01 0.00    1.24   -6.40\n",
      "Step 21 Reward:-0.2627 Losses: 6.49 0.01 0.00    1.12   -5.96\n",
      "Step 22 Reward:-0.2750 Losses: 5.05 0.01 0.00    0.98   -5.51\n",
      "Step 23 Reward:-0.2199 Losses: 4.67 0.01 0.00    0.82   -4.76\n",
      "Step 24 Reward:-0.3647 Losses: 6.84 0.01 0.00    0.68   -4.10\n",
      "Step 25 Reward:-0.5614 Losses: 5.61 0.01 0.00    0.56   -3.49\n",
      "Step 26 Reward:-0.4369 Losses: 5.71 0.01 0.00    0.47   -2.89\n",
      "Step 27 Reward:-0.6172 Losses: 6.51 0.01 0.00    0.42   -2.47\n",
      "Step 28 Reward:-0.4008 Losses: 6.01 0.01 0.00    0.39   -2.21\n",
      "Step 29 Reward:-0.3698 Losses: 4.76 0.01 0.00    0.38   -2.07\n",
      "Step 30 Reward:-0.4379 Losses: 5.19 0.01 0.00    0.39   -2.02\n",
      "Step 31 Reward:-0.4940 Losses: 5.36 0.02 0.00    0.41   -2.05\n",
      "Step 32 Reward:-0.5124 Losses: 5.36 0.02 0.00    0.44   -2.20\n",
      "Step 33 Reward:-0.4491 Losses: 6.22 0.02 0.00    0.46   -2.48\n",
      "Step 34 Reward:-0.2170 Losses: 5.05 0.02 0.00    0.47   -2.72\n",
      "Step 35 Reward:-0.3904 Losses: 4.56 0.03 0.00    0.48   -3.10\n",
      "Step 36 Reward:-0.3961 Losses: 4.53 0.03 0.00    0.49   -3.51\n",
      "Step 37 Reward:-0.3997 Losses: 3.35 0.02 0.00    0.51   -3.92\n",
      "Step 38 Reward:-0.3170 Losses: 5.57 0.03 0.00    0.52   -4.22\n",
      "Step 39 Reward:-0.1200 Losses: 7.12 0.05 0.00    0.53   -4.40\n",
      "Step 40 Reward:-0.1448 Losses: 6.25 0.05 0.01    0.53   -4.42\n",
      "Step 41 Reward:-0.2568 Losses: 6.20 0.05 0.01    0.52   -4.31\n",
      "Step 42 Reward:-0.2365 Losses: 5.65 0.06 0.01    0.51   -4.13\n",
      "Step 43 Reward:-0.4675 Losses: 4.55 0.05 0.01    0.49   -3.89\n",
      "Step 44 Reward:-0.3181 Losses: 5.67 0.07 0.01    0.47   -3.50\n",
      "Step 45 Reward:-0.3948 Losses: 5.19 0.08 0.01    0.46   -3.35\n",
      "Step 46 Reward:-0.2284 Losses: 4.70 0.09 0.01    0.43   -3.02\n",
      "Step 47 Reward:-0.0705 Losses: 4.31 0.10 0.01    0.42   -2.76\n",
      "Step 48 Reward:-0.5617 Losses: 4.23 0.11 0.01    0.41   -2.66\n",
      "Step 49 Reward:-0.4635 Losses: 5.56 0.11 0.01    0.40   -2.52\n",
      "Step 50 Reward:-0.4221 Losses: 7.17 0.15 0.01    0.39   -2.52\n",
      "Step 51 Reward:-0.5691 Losses: 4.54 0.11 0.01    0.39   -2.45\n",
      "Step 52 Reward:-0.7390 Losses: 5.96 0.17 0.02    0.39   -2.49\n",
      "Step 53 Reward:-0.4209 Losses: 5.41 0.17 0.02    0.40   -2.59\n",
      "Step 54 Reward:-0.6701 Losses: 5.13 0.17 0.02    0.41   -2.75\n",
      "Step 55 Reward:-0.5121 Losses: 5.79 0.20 0.02    0.42   -2.89\n",
      "Step 56 Reward:-0.0144 Losses: 5.36 0.18 0.02    0.44   -3.18\n",
      "Step 57 Reward:-0.0728 Losses: 4.47 0.16 0.02    0.45   -3.38\n",
      "Step 58 Reward:-0.3785 Losses: 7.51 0.20 0.02    0.47   -3.67\n",
      "Step 59 Reward:-0.6125 Losses: 4.26 0.14 0.01    0.47   -3.75\n",
      "Step 60 Reward:-0.5745 Losses: 5.33 0.15 0.02    0.47   -3.74\n",
      "Step 61 Reward:-0.3774 Losses: 5.85 0.17 0.02    0.47   -3.89\n",
      "Step 62 Reward:-0.6082 Losses: 4.44 0.17 0.02    0.46   -3.75\n",
      "Step 63 Reward:-0.3328 Losses: 5.84 0.21 0.02    0.46   -3.64\n",
      "Step 64 Reward:-0.4951 Losses: 4.68 0.19 0.02    0.45   -3.46\n",
      "Step 65 Reward:-0.5421 Losses: 4.80 0.22 0.02    0.44   -3.31\n",
      "Step 66 Reward:-0.3104 Losses: 4.05 0.19 0.02    0.42   -3.09\n",
      "Step 67 Reward:-0.5270 Losses: 3.04 0.17 0.02    0.41   -2.87\n",
      "Step 68 Reward:-0.4480 Losses: 4.13 0.18 0.02    0.40   -2.85\n",
      "Step 69 Reward:-0.4457 Losses: 3.46 0.19 0.02    0.40   -2.83\n",
      "Step 70 Reward:-0.4382 Losses: 4.27 0.22 0.02    0.40   -3.04\n",
      "Step 71 Reward:-0.4356 Losses: 5.49 0.21 0.02    0.42   -3.24\n",
      "Step 72 Reward:-0.5354 Losses: 4.05 0.16 0.02    0.43   -3.42\n",
      "Step 73 Reward:-0.3693 Losses: 3.86 0.16 0.02    0.45   -3.65\n",
      "Step 74 Reward:-0.2345 Losses: 3.22 0.20 0.02    0.46   -3.79\n",
      "Step 75 Reward:-0.4153 Losses: 2.92 0.29 0.03    0.46   -3.85\n",
      "Step 76 Reward:-0.3205 Losses: 3.78 0.20 0.02    0.47   -4.11\n",
      "Step 77 Reward:-0.3939 Losses: 3.53 0.18 0.02    0.47   -4.14\n",
      "Step 78 Reward:-0.3398 Losses: 4.75 0.25 0.03    0.46   -4.12\n",
      "Step 79 Reward:-0.0457 Losses: 5.05 0.25 0.03    0.45   -3.77\n",
      "Step 80 Reward:-0.0542 Losses: 5.10 0.28 0.03    0.44   -3.50\n",
      "Step 81 Reward:-0.2480 Losses: 4.73 0.27 0.03    0.42   -3.12\n",
      "Step 82 Reward:-0.3576 Losses: 6.20 0.34 0.03    0.40   -2.84\n",
      "Step 83 Reward:-0.3403 Losses: 5.91 0.42 0.04    0.39   -2.63\n",
      "Step 84 Reward:-0.2881 Losses: 5.38 0.41 0.04    0.37   -2.54\n",
      "Step 85 Reward:-0.5550 Losses: 4.54 0.59 0.06    0.37   -2.55\n",
      "Step 86 Reward:-0.3994 Losses: 5.58 0.45 0.05    0.37   -2.60\n",
      "Step 87 Reward:-0.5505 Losses: 5.60 0.50 0.05    0.38   -2.84\n",
      "Step 88 Reward:-0.4582 Losses: 4.49 0.36 0.04    0.39   -2.89\n",
      "Step 89 Reward:-0.3564 Losses: 4.88 0.28 0.03    0.41   -3.16\n",
      "Step 90 Reward:-0.5473 Losses: 2.78 0.31 0.03    0.44   -3.39\n",
      "Step 91 Reward: 0.1491 Losses: 4.58 0.35 0.03    0.47   -3.81\n",
      "Step 92 Reward: 0.2167 Losses: 5.21 0.43 0.04    0.47   -3.91\n",
      "Step 93 Reward:-0.3645 Losses: 5.57 0.35 0.04    0.44   -3.74\n",
      "Step 94 Reward:-0.6521 Losses: 4.09 0.29 0.03    0.42   -3.45\n",
      "Step 95 Reward:-0.5157 Losses: 5.40 0.29 0.03    0.40   -3.08\n",
      "Step 96 Reward:-0.3151 Losses: 3.96 0.25 0.02    0.39   -2.71\n",
      "Step 97 Reward:-0.6446 Losses: 3.02 0.22 0.02    0.39   -2.62\n",
      "Step 98 Reward:-0.3884 Losses: 3.78 0.23 0.02    0.40   -2.84\n",
      "Step 99 Reward:-0.4297 Losses: 3.55 0.24 0.02    0.39   -2.93\n",
      "Step 100 Reward:-0.3097 Losses: 3.92 0.25 0.03    0.40   -3.11\n",
      "Step 101 Reward:-0.3935 Losses: 3.04 0.23 0.02    0.41   -3.21\n",
      "Step 102 Reward:-0.5011 Losses: 4.43 0.23 0.02    0.42   -3.61\n",
      "Step 103 Reward:-0.3955 Losses: 3.36 0.21 0.02    0.42   -3.64\n",
      "Step 104 Reward:-0.6181 Losses: 3.51 0.20 0.02    0.43   -3.60\n",
      "Step 105 Reward:-0.4923 Losses: 3.20 0.21 0.02    0.42   -3.54\n",
      "Step 106 Reward:-0.0329 Losses: 3.63 0.24 0.02    0.41   -3.46\n",
      "Step 107 Reward:-0.1414 Losses: 4.21 0.26 0.03    0.39   -3.21\n",
      "Step 108 Reward:-0.3519 Losses: 4.11 0.29 0.03    0.39   -3.22\n",
      "Step 109 Reward:-0.5280 Losses: 5.32 0.32 0.03    0.37   -2.95\n",
      "Step 110 Reward:-0.4039 Losses: 3.50 0.22 0.02    0.37   -3.04\n",
      "Step 111 Reward:-0.4630 Losses: 3.55 0.26 0.03    0.37   -3.01\n",
      "Step 112 Reward:-0.6197 Losses: 2.97 0.22 0.02    0.38   -3.05\n",
      "Step 113 Reward:-0.1773 Losses: 2.77 0.25 0.02    0.37   -2.94\n",
      "Step 114 Reward: 0.0000 Losses: 3.82 0.31 0.03    0.39   -3.17\n",
      "Step 115 Reward: 0.1240 Losses: 4.17 0.28 0.03    0.38   -3.13\n",
      "Step 116 Reward:-0.4866 Losses: 3.84 0.24 0.02    0.38   -3.26\n",
      "Step 117 Reward:-0.5737 Losses: 5.83 0.33 0.03    0.39   -3.20\n",
      "Step 118 Reward:-0.5581 Losses: 5.44 0.31 0.03    0.38   -3.20\n",
      "Step 119 Reward:-0.6609 Losses: 3.64 0.26 0.03    0.39   -3.41\n",
      "Step 120 Reward:-0.2515 Losses: 3.12 0.27 0.03    0.40   -3.32\n",
      "Step 121 Reward: 0.2147 Losses: 3.25 0.26 0.03    0.40   -3.51\n",
      "Step 122 Reward:-0.4214 Losses: 3.28 0.25 0.03    0.40   -3.29\n",
      "Step 123 Reward:-0.6139 Losses: 3.00 0.23 0.02    0.41   -3.48\n",
      "Step 124 Reward:-0.3145 Losses: 5.66 0.31 0.03    0.40   -3.35\n",
      "Step 125 Reward:-0.2175 Losses: 3.87 0.30 0.03    0.39   -3.18\n",
      "Step 126 Reward:-0.4670 Losses: 3.26 0.31 0.03    0.37   -3.03\n",
      "Step 127 Reward:-0.1803 Losses: 3.64 0.30 0.03    0.38   -3.00\n",
      "Step 128 Reward:-0.0172 Losses: 3.61 0.35 0.04    0.38   -2.87\n",
      "Step 129 Reward:-0.1868 Losses: 4.33 0.32 0.03    0.37   -2.92\n",
      "Step 130 Reward:-0.3351 Losses: 4.92 0.36 0.04    0.37   -2.65\n",
      "Step 131 Reward:-0.3578 Losses: 4.77 0.39 0.04    0.34   -2.55\n",
      "Step 132 Reward:-0.3619 Losses: 3.66 0.32 0.03    0.35   -2.58\n",
      "Step 133 Reward:-0.5703 Losses: 3.43 0.37 0.04    0.36   -2.63\n",
      "Step 134 Reward:-0.3937 Losses: 2.48 0.33 0.03    0.39   -2.82\n",
      "Step 135 Reward:-0.4127 Losses: 3.80 0.32 0.03    0.40   -3.06\n",
      "Step 136 Reward:-0.5627 Losses: 5.56 0.38 0.04    0.40   -3.22\n",
      "Step 137 Reward:-0.3107 Losses: 3.81 0.35 0.03    0.41   -3.21\n",
      "Step 138 Reward:-0.3451 Losses: 3.16 0.33 0.03    0.41   -3.39\n",
      "Step 139 Reward:-0.3492 Losses: 3.25 0.32 0.03    0.41   -3.12\n",
      "Step 140 Reward:-0.3128 Losses: 4.30 0.38 0.04    0.37   -2.81\n",
      "Step 141 Reward:-0.4620 Losses: 3.97 0.31 0.03    0.38   -2.57\n",
      "Step 142 Reward:-0.3091 Losses: 3.26 0.31 0.03    0.37   -2.47\n",
      "Step 143 Reward:-0.4863 Losses: 3.71 0.32 0.03    0.36   -2.28\n",
      "Step 144 Reward:-0.3833 Losses: 4.35 0.46 0.05    0.35   -2.38\n",
      "Step 145 Reward:-0.5767 Losses: 3.91 0.39 0.04    0.36   -2.46\n",
      "Step 146 Reward:-0.6282 Losses: 3.59 0.35 0.03    0.38   -2.54\n",
      "Step 147 Reward:-0.2772 Losses: 3.74 0.37 0.04    0.39   -2.68\n",
      "Step 148 Reward:-0.5073 Losses: 4.08 0.41 0.04    0.40   -2.72\n",
      "Step 149 Reward:-0.4340 Losses: 4.87 0.45 0.04    0.39   -2.73\n",
      "Step 150 Reward:-0.5271 Losses: 2.83 0.38 0.04    0.40   -2.86\n",
      "Step 151 Reward:-0.6197 Losses: 3.24 0.39 0.04    0.40   -2.90\n",
      "Step 152 Reward:-0.5005 Losses: 2.92 0.31 0.03    0.41   -2.97\n",
      "Step 153 Reward:-0.6492 Losses: 2.71 0.34 0.03    0.40   -2.84\n",
      "Step 154 Reward:-0.5629 Losses: 3.83 0.38 0.04    0.39   -2.77\n",
      "Step 155 Reward:-0.5318 Losses: 2.63 0.35 0.03    0.40   -2.65\n",
      "Step 156 Reward:-0.5590 Losses: 4.29 0.40 0.04    0.39   -2.55\n",
      "Step 157 Reward:-0.4401 Losses: 3.44 0.41 0.04    0.40   -2.73\n",
      "Step 158 Reward:-0.5657 Losses: 4.39 0.40 0.04    0.42   -2.69\n",
      "Step 159 Reward:-0.4883 Losses: 3.01 0.34 0.03    0.42   -3.16\n",
      "Step 160 Reward:-0.5009 Losses: 3.53 0.39 0.04    0.43   -3.28\n",
      "Step 161 Reward:-0.5718 Losses: 3.27 0.44 0.04    0.43   -3.16\n",
      "Step 162 Reward:-0.5027 Losses: 3.97 0.43 0.04    0.42   -3.01\n",
      "Step 163 Reward:-0.6152 Losses: 2.63 0.39 0.04    0.41   -2.79\n",
      "Step 164 Reward:-0.4956 Losses: 3.38 0.39 0.04    0.39   -2.66\n",
      "Step 165 Reward:-0.5995 Losses: 3.01 0.34 0.03    0.38   -2.41\n",
      "Step 166 Reward:-0.6940 Losses: 2.93 0.40 0.04    0.36   -2.35\n",
      "Step 167 Reward:-0.4624 Losses: 2.22 0.41 0.04    0.37   -2.40\n",
      "Step 168 Reward:-0.5421 Losses: 2.64 0.42 0.04    0.38   -2.54\n",
      "Step 169 Reward:-0.5043 Losses: 2.60 0.37 0.04    0.40   -2.87\n",
      "Step 170 Reward:-0.5170 Losses: 2.86 0.40 0.04    0.42   -3.15\n",
      "Step 171 Reward:-0.6851 Losses: 3.13 0.41 0.04    0.44   -3.23\n",
      "Step 172 Reward:-0.4819 Losses: 3.15 0.35 0.04    0.44   -3.29\n",
      "Step 173 Reward:-0.4582 Losses: 2.50 0.37 0.04    0.43   -3.38\n",
      "Step 174 Reward:-0.4212 Losses: 4.09 0.42 0.04    0.44   -3.19\n",
      "Step 175 Reward:-0.4740 Losses: 3.62 0.44 0.04    0.42   -2.79\n",
      "Step 176 Reward:-0.4751 Losses: 3.96 0.38 0.04    0.40   -2.54\n",
      "Step 177 Reward:-0.3481 Losses: 3.25 0.37 0.04    0.39   -2.46\n",
      "Step 178 Reward:-0.3119 Losses: 2.32 0.33 0.03    0.36   -2.32\n",
      "Step 179 Reward:-0.4111 Losses: 3.15 0.35 0.04    0.36   -2.13\n",
      "Step 180 Reward:-0.3816 Losses: 3.28 0.40 0.04    0.37   -2.43\n",
      "Step 181 Reward:-0.5694 Losses: 3.47 0.36 0.04    0.40   -2.60\n",
      "Step 182 Reward:-0.4706 Losses: 3.42 0.37 0.04    0.42   -2.96\n",
      "Step 183 Reward:-0.6396 Losses: 3.78 0.37 0.04    0.43   -2.97\n",
      "Step 184 Reward:-0.6899 Losses: 2.78 0.33 0.03    0.44   -3.24\n",
      "Step 185 Reward:-0.5001 Losses: 2.02 0.30 0.03    0.45   -3.28\n",
      "Step 186 Reward:-0.6092 Losses: 2.46 0.32 0.03    0.43   -3.31\n",
      "Step 187 Reward:-0.4230 Losses: 3.34 0.36 0.04    0.41   -3.06\n",
      "Step 188 Reward:-0.3449 Losses: 1.77 0.31 0.03    0.41   -2.89\n",
      "Step 189 Reward:-0.4549 Losses: 2.86 0.31 0.03    0.41   -2.71\n",
      "Step 190 Reward:-0.2617 Losses: 3.38 0.40 0.04    0.40   -2.67\n",
      "Step 191 Reward:-0.4032 Losses: 2.46 0.32 0.03    0.38   -2.30\n",
      "Step 192 Reward:-0.3618 Losses: 3.34 0.36 0.04    0.36   -2.28\n",
      "Step 193 Reward:-0.4610 Losses: 2.48 0.28 0.03    0.38   -2.59\n",
      "Step 194 Reward:-0.5432 Losses: 3.45 0.39 0.04    0.39   -2.68\n",
      "Step 195 Reward:-0.3138 Losses: 2.94 0.32 0.03    0.40   -2.78\n",
      "Step 196 Reward:-0.5210 Losses: 2.24 0.30 0.03    0.43   -3.01\n",
      "Step 197 Reward:-0.4609 Losses: 3.02 0.35 0.04    0.43   -3.15\n",
      "Step 198 Reward:-0.5643 Losses: 3.79 0.34 0.03    0.45   -3.32\n",
      "Step 199 Reward:-0.7031 Losses: 1.91 0.27 0.03    0.45   -3.24\n",
      "Step 200 Reward:-0.4143 Losses: 3.07 0.30 0.03    0.43   -3.30\n",
      "Step 201 Reward:-0.5280 Losses: 3.89 0.35 0.03    0.41   -2.80\n",
      "Step 202 Reward:-0.4483 Losses: 2.14 0.35 0.03    0.39   -2.43\n",
      "Step 203 Reward:-0.3559 Losses: 2.73 0.31 0.03    0.39   -2.30\n",
      "Step 204 Reward:-0.4743 Losses: 3.54 0.33 0.03    0.39   -2.43\n",
      "Step 205 Reward:-0.3411 Losses: 2.63 0.34 0.03    0.40   -2.70\n",
      "Step 206 Reward:-0.4226 Losses: 3.28 0.34 0.03    0.40   -2.84\n",
      "Step 207 Reward:-0.4301 Losses: 2.63 0.38 0.04    0.41   -3.12\n",
      "Step 208 Reward:-0.3053 Losses: 2.48 0.30 0.03    0.44   -3.28\n",
      "Step 209 Reward:-0.5371 Losses: 2.63 0.28 0.03    0.42   -3.14\n",
      "Step 210 Reward:-0.4552 Losses: 2.66 0.29 0.03    0.42   -2.85\n",
      "Step 211 Reward:-0.6591 Losses: 2.28 0.28 0.03    0.41   -3.02\n",
      "Step 212 Reward:-0.6404 Losses: 3.80 0.33 0.03    0.39   -2.52\n",
      "Step 213 Reward:-0.4034 Losses: 3.48 0.35 0.03    0.37   -2.49\n",
      "Step 214 Reward:-0.5061 Losses: 1.71 0.28 0.03    0.37   -2.58\n",
      "Step 215 Reward:-0.3093 Losses: 2.45 0.32 0.03    0.38   -2.71\n",
      "Step 216 Reward:-0.3898 Losses: 2.84 0.35 0.04    0.38   -2.62\n",
      "Step 217 Reward:-0.5763 Losses: 2.71 0.32 0.03    0.38   -2.72\n",
      "Step 218 Reward:-0.0844 Losses: 2.97 0.36 0.04    0.37   -2.52\n",
      "Step 219 Reward:-0.7606 Losses: 3.39 0.35 0.04    0.39   -2.70\n",
      "Step 220 Reward:-0.3940 Losses: 1.86 0.27 0.03    0.41   -3.06\n",
      "Step 221 Reward:-0.5964 Losses: 1.86 0.31 0.03    0.40   -2.91\n",
      "Step 222 Reward:-0.2673 Losses: 2.47 0.32 0.03    0.40   -2.94\n",
      "Step 223 Reward:-0.5213 Losses: 2.78 0.32 0.03    0.41   -3.00\n",
      "Step 224 Reward:-0.4692 Losses: 2.91 0.36 0.04    0.40   -2.73\n",
      "Step 225 Reward:-0.3868 Losses: 2.26 0.29 0.03    0.39   -2.64\n",
      "Step 226 Reward:-0.5245 Losses: 1.66 0.29 0.03    0.39   -2.59\n",
      "Step 227 Reward:-0.2550 Losses: 2.48 0.35 0.03    0.39   -2.63\n",
      "Step 228 Reward:-0.4582 Losses: 2.88 0.32 0.03    0.40   -2.70\n",
      "Step 229 Reward:-0.4336 Losses: 2.30 0.31 0.03    0.40   -2.69\n",
      "Step 230 Reward:-0.5586 Losses: 2.72 0.31 0.03    0.42   -2.70\n",
      "Step 231 Reward:-0.6462 Losses: 2.96 0.36 0.04    0.42   -2.63\n",
      "Step 232 Reward:-0.4531 Losses: 1.80 0.28 0.03    0.43   -3.02\n",
      "Step 233 Reward:-0.6406 Losses: 2.55 0.32 0.03    0.42   -2.96\n",
      "Step 234 Reward:-0.5696 Losses: 2.68 0.31 0.03    0.39   -2.68\n",
      "Step 235 Reward:-0.5427 Losses: 2.88 0.38 0.04    0.39   -2.60\n",
      "Step 236 Reward:-0.7145 Losses: 2.09 0.31 0.03    0.38   -2.14\n",
      "Step 237 Reward:-0.4463 Losses: 1.84 0.33 0.03    0.39   -2.44\n",
      "Step 238 Reward:-0.5164 Losses: 2.24 0.32 0.03    0.41   -2.76\n",
      "Step 239 Reward:-0.4884 Losses: 2.65 0.33 0.03    0.42   -3.06\n",
      "Step 240 Reward:-0.4204 Losses: 2.41 0.30 0.03    0.43   -2.99\n",
      "Step 241 Reward:-0.6225 Losses: 2.87 0.37 0.04    0.43   -3.21\n",
      "Step 242 Reward:-0.4553 Losses: 3.76 0.37 0.04    0.40   -2.87\n",
      "Step 243 Reward:-0.6848 Losses: 3.01 0.37 0.04    0.39   -2.51\n",
      "Step 244 Reward:-0.6863 Losses: 2.94 0.35 0.03    0.37   -2.41\n",
      "Step 245 Reward:-0.3220 Losses: 1.66 0.29 0.03    0.37   -2.48\n",
      "Step 246 Reward:-0.4884 Losses: 1.96 0.34 0.03    0.37   -2.42\n",
      "Step 247 Reward:-0.3694 Losses: 3.09 0.40 0.04    0.41   -2.42\n",
      "Step 248 Reward:-0.5525 Losses: 1.92 0.29 0.03    0.41   -2.96\n",
      "Step 249 Reward:-0.5955 Losses: 2.29 0.35 0.03    0.43   -3.16\n",
      "Step 250 Reward:-0.4433 Losses: 1.92 0.30 0.03    0.43   -3.29\n",
      "Step 251 Reward:-0.5946 Losses: 2.57 0.35 0.03    0.42   -3.17\n",
      "Step 252 Reward:-0.4841 Losses: 2.11 0.39 0.04    0.41   -3.04\n",
      "Step 253 Reward:-0.4549 Losses: 2.05 0.36 0.04    0.39   -2.73\n",
      "Step 254 Reward:-0.4582 Losses: 1.85 0.33 0.03    0.39   -2.77\n",
      "Step 255 Reward:-0.2154 Losses: 3.14 0.45 0.05    0.39   -2.79\n",
      "Step 256 Reward:-0.4540 Losses: 2.34 0.32 0.03    0.42   -3.09\n",
      "Step 257 Reward:-0.3594 Losses: 2.22 0.31 0.03    0.44   -3.41\n",
      "Step 258 Reward:-0.5070 Losses: 2.15 0.33 0.03    0.45   -3.65\n",
      "Step 259 Reward:-0.6982 Losses: 2.09 0.34 0.03    0.45   -3.69\n",
      "Step 260 Reward:-0.3169 Losses: 2.42 0.34 0.03    0.43   -3.42\n",
      "Step 261 Reward:-0.4785 Losses: 1.95 0.31 0.03    0.44   -3.43\n",
      "Step 262 Reward:-0.4150 Losses: 2.19 0.36 0.04    0.42   -3.06\n",
      "Step 263 Reward:-0.3310 Losses: 2.16 0.34 0.03    0.40   -2.88\n",
      "Step 264 Reward:-0.4840 Losses: 1.86 0.34 0.03    0.37   -2.44\n",
      "Step 265 Reward:-0.3846 Losses: 4.05 0.47 0.05    0.36   -2.51\n",
      "Step 266 Reward:-0.6651 Losses: 1.74 0.31 0.03    0.36   -2.47\n",
      "Step 267 Reward:-0.5624 Losses: 1.73 0.29 0.03    0.37   -2.56\n",
      "Step 268 Reward:-0.3968 Losses: 1.72 0.30 0.03    0.40   -2.92\n",
      "Step 269 Reward:-0.4842 Losses: 2.52 0.40 0.04    0.41   -3.08\n"
     ]
    }
   ],
   "source": [
    "for step in range(number_of_training_steps):\n",
    "    rew = dreamer.generate_samples(env_step_per_training_step)\n",
    "\n",
    "    world_loss, critic_loss, actor_loss = dreamer.train(batch_size, seq_len)\n",
    "    world_losses.append(world_loss)\n",
    "    critic_losses.append(critic_loss)\n",
    "    actor_losses.append(actor_loss)\n",
    "    rewards.append(rew)\n",
    "    print(f\"Step {step} Reward:{rew: .4f} Losses: {world_loss[0].item():.2f} {world_loss[1].item():.2f} {world_loss[2].item():.2f}    {critic_loss:.2f}   {actor_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Plot the rewards\n",
    "#plt.plot(rewards)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n",
      "C:\\Users\\pmaty\\AppData\\Local\\Temp\\ipykernel_22232\\2294970489.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs_0 = torch.tensor(obs_0, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n",
      "True False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m z_0 \u001b[38;5;241m=\u001b[39m dreamer\u001b[38;5;241m.\u001b[39mworld_model\u001b[38;5;241m.\u001b[39mget_latent(h_0, obs_0)\n\u001b[0;32m     17\u001b[0m a_enemy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m a_0 \u001b[38;5;241m=\u001b[39m \u001b[43mdreamer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#a_0 = torch.tensor([1, 0.0, 0.0, 0.0], dtype=torch.float32, device=device)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m a_0_numpy \u001b[38;5;241m=\u001b[39m a_0\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\pmaty\\Desktop\\RL_TUE\\HW\\rl-hockey-homework\\dreamer\\actor.py:57\u001b[0m, in \u001b[0;36mActor.get_action\u001b[1;34m(self, h, z)\u001b[0m\n\u001b[0;32m     54\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     56\u001b[0m dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(probs\u001b[38;5;241m=\u001b[39mprobs)\n\u001b[1;32m---> 57\u001b[0m sampled \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m hard_sampled \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mone_hot(sampled, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_bins)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     59\u001b[0m a \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_value_from_distribution(hard_sampled, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pmaty\\miniconda3\\envs\\rl_hw\\Lib\\site-packages\\torch\\distributions\\categorical.py:134\u001b[0m, in \u001b[0;36mCategorical.sample\u001b[1;34m(self, sample_shape)\u001b[0m\n\u001b[0;32m    132\u001b[0m     sample_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize(sample_shape)\n\u001b[0;32m    133\u001b[0m probs_2d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events)\n\u001b[1;32m--> 134\u001b[0m samples_2d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples_2d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "obs_agent2 = env.obs_agent_two()\n",
    "_ = env.render()\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "\tenv_reset = True\n",
    "\tfor _ in range(6000):\n",
    "\t\tif env_reset:\n",
    "\t\t\tobs_0, info = env.reset()\n",
    "\t\t\th_0 = dreamer.world_model.get_default_hidden()\n",
    "\t\t\tenv_reset = False\n",
    "\t\tenv.render(mode=\"human\")\n",
    "\n",
    "\t\tobs_0 = torch.tensor(obs_0, dtype=torch.float32, device=device)\n",
    "\t\tz_0 = dreamer.world_model.get_latent(h_0, obs_0)\n",
    "\t\ta_enemy = np.random.uniform(-1,1,4)\n",
    "\t\ta_0 = dreamer.actor.get_action(h_0, z_0)\n",
    "\t\t#a_0 = torch.tensor([1, 0.0, 0.0, 0.0], dtype=torch.float32, device=device)\n",
    "\t\ta_0_numpy = a_0.cpu().detach().numpy()\n",
    "\t\tobs_1, r, done, t, info = env.step(np.hstack([a_0_numpy,a_enemy]))    \n",
    "\t\th_0 = dreamer.world_model.get_recurrent_hidden(h_0, z_0, a_0)\n",
    "\t\tif done or t:\n",
    "\t\t\tprint(done, t)\n",
    "\t\t\tbreak\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
