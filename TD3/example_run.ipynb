{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TD3Agent' object has no attribute 'select_action'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../hockey_env/hockey\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhockey_env\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mh_env\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain_td3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_td3\n",
      "File \u001b[1;32mc:\\Users\\gross\\Documents\\rl-hockey-homework\\TD3\\train_td3.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m total_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Select action with exploration noise\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m(np\u001b[38;5;241m.\u001b[39marray(state))\n\u001b[0;32m     49\u001b[0m action \u001b[38;5;241m=\u001b[39m (action \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, EXPLORE_NOISE, size\u001b[38;5;241m=\u001b[39maction_dim))\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;241m-\u001b[39mmax_action, max_action)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Step environment\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TD3Agent' object has no attribute 'select_action'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from td3_agent import TD3Agent\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../hockey_env/hockey'))\n",
    "import hockey_env as h_env\n",
    "from train_td3 import train_td3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model path\n",
    "model_path = \"TD3_Hockey_1000.pth\"\n",
    "\n",
    "# If no saved model exists, train from scratch\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"ðŸš€ No trained model found! Starting training from scratch...\")\n",
    "    train_td3(num_episodes=1000, save_every=100, opponent=\"weak\")  # Options: \"weak\", \"strong\", \"td3\"\n",
    "else:\n",
    "    print(f\"âœ… Found trained model: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Environment (Change opponent type as needed)\n",
    "env = HockeyEnv_BasicOpponent(weak_opponent=True)  # Choose: weak_opponent=True / False\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "# Create TD3 Agent\n",
    "agent = TD3Agent(state_dim, action_dim, max_action)\n",
    "\n",
    "# Load trained model if available\n",
    "if os.path.exists(model_path):\n",
    "    agent.actor.load_state_dict(torch.load(model_path))  # Load latest trained model\n",
    "    print(f\"ðŸ“¥ Loaded trained model from {model_path}\")\n",
    "else:\n",
    "    print(\"ðŸš€ No trained model found, starting fresh!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    env.render(mode=\"human\")  # Visualize game\n",
    "    action = agent.select_action(obs)  # TD3 selects action\n",
    "    opponent_action = env.opponent.act(env.obs_agent_two())  # AI Opponent\n",
    "    obs, reward, done, _, _ = env.step(np.hstack([action, opponent_action]))\n",
    "\n",
    "env.close()  # Close environment after game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train in Shooting Mode\n",
    "train_td3(num_episodes=500, save_every=50, opponent=\"weak\")  # Train TD3 in shooting mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train in Defense Mode\n",
    "train_td3(num_episodes=500, save_every=50, opponent=\"strong\")  # Train TD3 in defense mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TD3 vs TD3 (Self-Play)\n",
    "train_td3(num_episodes=1000, save_every=100, opponent=\"td3\")  # TD3 vs TD3 self-play training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-hockey",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
